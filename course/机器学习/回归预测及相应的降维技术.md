# 6 回归预测及相应的降维技术

+ [回归分析](#3.1-回归分析)
+ [变量筛选技术](#3.2-变量筛选技术)
+ [降维技术](#3.3-降维技术)

## 6.1 回归分析

+ [参考书](#3.1.1-参考书)
+ [一元线性回归分析](#3.1.2-一元线性回归分析)
+ [多元线性回归分析](#3.1.3-多元线性回归分析)
+ [广义线性模型](#3.1.4-广义线性模型)
+ [案例分析-预测网页流量](#3.1.5-案例分析-预测网页流量)

### 6.1.1 参考书

多元统计与R语言

### 6.1.2 一元线性回归分析

```
y = c(61,57,58,40,90,35,68)
x = c(170,168,175,153,185,135,172)
plot(x, y)
z = lm(y~x+1)
summary(z)
plot(z)
```
- 过原点的线性模型
```
z = lm(y~x-1)
summary(z)
plot(z)
```
- 计算残差平方和

```
deviance(a)
```
- 做出预测

```
z = data.frame(x = 185)
predict(a, z)
```

### 6.1.3 多元线性回归分析

- Swiss数据集(Swiss Fertility and Socioeconomic Indicators)

```
swiss.lm = lm(Fertility~., data = swiss)
summary(swiss.lm)
```

#### 6.1.3.1 虚拟变量

isman
iswoman

#### 6.1.3.2 逐步回归(RSS, R**2, AIC)

```
s1 = step(s, direction = "both")
drop1()
```

#### 6.1.3.3 回归诊断
样本是否符合正态分布假设
是否存在离群值导致模型产生较大误差
线性模型是否合理
误差是否满足独立性、等方差、正态分布等假设条件
是否存在多重共线性

##### 6.1.3.3.1 正态分布检验
```
shapio.test(x$x1)
shapio.test(x$x3)
```
P > 0.05, 正态性分布

##### 6.1.3.3.2 残差分析
```
residuals(lm.sol)
```
iris数据集
```
z <- lm(iris$Sepal.Length ~ iris$Sepal.Width)
plot(z)
```
##### 6.1.3.3.3 多重共线性
```
Kappa()
```
Kappa < 100, 不存在多重共线性
Kappa < 1000, 存在轻度多重共线性
Kappa > 1000, 存在严重多重共线性

多重共线性的变量
```(矩阵特征根)
eigen()
```
### 6.1.4 广义线性模型

#### 6.1.4.1 对数法
```
lm.log = lm(y~log(x))
```
#### 6.1.4.2 指数法
```
lm.exp = lm(log(y)~x)
```
#### 6.1.4.3 幂函数
```
lm.pow = lm(log(y)~log(x))
```
#### 6.1.4.4 多项式回归

#### 6.1.4.5 Logistic回归
```
norell$Ymat <- cbind(norell$success, norell$n-norell$success)
glm.sol <- glm(Ymat~x, family = binomial, data = norell)
summary(glm.sol)
```
#### 6.1.4.6 Poisson分布

### 6.1.5 案例分析-预测网页流量
使用互联网排名前1000的网站的数据
Rank:排名,
PageView:网站访问量,
UniqueVisitor:访问用户数量,
HasAdvertising:是否有广告,
IsEnglish:主要使用的语言是否为英语

```
top.1000.sites <- read.csv('data/top_1000_sites.tsv', sep = '\t', stringsAsFactors = FALSE)
ggplot(top.1000.sites, aes(x = log(PageViews), y = log(UniqueVisitors))) + geom_point()
lm.fit <- lm(log(PageViews) ~ log(UniqueVisitors, data = top.1000.sites))
lm.fit <- lm(log(PageViews) ~ HasAdvertising + log(UniqueVisitors) + InEnglish, data = top.1000.sites)
```

## 6.2 变量筛选技术

+ [岭回归](#3.2.1-岭回归(ridge-regression,-rr))
+ [LASSO](#3.2.2-lasso(the-least-absolute-shrinkage-and-selectionator-operator))

### 6.2.1 岭回归(Ridge Regression, RR)
longley数据集
```
library(MASS)
summary(fm1 <- lm(Employed ~ ., data = longley))
names(longley)[1] <- "y"
lm.ridge(y ~ ., longley)
plot(lm.ridge(y ~ ., longley, lambda = seq(0, 0.1, 0.001)))
select(lm.ridge(y ~ ., longley, lambda = seq(0, 0.1, 0.001)))
```
R的ridge包
```
liberary(ridge)
a = linearRidge(GNP.deflator ~ ., data = longley)
summary(a)
```

#### 6.2.1.1 岭迹图
通过岭迹图观察岭估计的情况，可以判断出应该剔除哪些变量

#### 6.2.1.2 岭参数的一般选择原则
各回归系数的岭估计基本稳定
用最小二乘估计时符号不合理的回归系数，其岭估计的符号变得合理
回归系数没有不合乎实际意义的绝对值
残差平方和增大不太多 

#### 6.2.1.3 岭回归的问题
岭参数计算方法太多，差异太大
根据岭迹图进行变量筛选，随意性太大
岭回归返回的模型（如果没有经过变量筛选）包含所有的变量

### 6.2.2 LASSO(The Least Absolute Shrinkage and Selectionator Operator)

#### 6.2.2.1 参考书
复杂数据统计方案-基于R的应用
The Elements of Statistical Learning

#### 6.2.2.2 最小角回归(LAR)
```
longley数据集
w = as.matrix(longley)
laa = lars(w[, 2: 7], w[, 1])
laa
plot(laa)
summary(laa)
```
## 6.3 降维技术

+ [主成分分析](#3.3.1-主成分分析)
+ [因子分析](#3.3.2-因子分析)
+ [应用：员工绩效考核指标设计](#3.3.3-应用：员工绩效考核指标设计)

### 6.3.1 主成分分析
```
student.pr <- princomp(student, cor = TRUE)
summary(student.pr, loadings = true)
predict(student.pr)
screeplot(student.pr, type = "lines")
```
#### 6.3.1.1 主成分分析的应用

##### 6.3.1.1.1 主成分得分-相当于predict()
```
PCA = princomp(X, cor = T)
PCA
PCA$loadings
screeplot(PCA, type = "lines")
```
##### 6.3.1.1.2 聚类(Clustering vector)
```
kmeans(PCA$score[, 1: 2], 5)
```
##### 6.3.1.1.3 主成分回归(排除多重共线性)
```
conomy.pr <- princomp(~ x1 + x2 + x3, data = conomy, cor = T)
summary(conomy.pr, loadings = TRUE)
```
### 6.3.2 因子分析
用于分析隐藏在表面现象背后的因子作用的统计模型，试图用最少个数的不可测的公共因子的线性函数与特殊因子之和来描述原来观测的每一分量
更重视相关变量的“共变异量”，组合的是相关性较强的原始变量，目的是找到在背后起作用的少量关键因子

#### 6.3.2.1 主成分法

#### 6.3.2.2 主因子法

#### 6.3.2.3 极大似然法
```
factanal(~ ., factors = 5, data = rt, rotation = "varimax")
```
#### 6.3.2.4 方差最大的正交旋转
```
vm1 <- varimax(x, normalize = TRUE, eps = 1e-5)
```
### 6.3.3 应用：员工绩效考核指标设计

### 第07章 线性回归算法
- 041 线性回归算法概述
- 042 误差项分析
- 043 似然函数求解
- 044 目标函数推导
- 045 线性回归求解

### 第08章 梯度下降算法
- 046 梯度下降原理
- 047 梯度下降方法对比
- 048 学习率对结果的影响

### 第09章 逻辑回归算法
- 049 逻辑回归算法原理推导
- 050 逻辑回归求解

### 第10章 案例实战:python实现逻辑回归与梯度下降策略
- 051 python实现逻辑回归任务概述
- 052 完成梯度下降模块
- 053 停止策略与梯度下降策略对比
- 054 实验对比效果

### 第11章 项目实战:案例实战信用卡欺诈检测
- 055 案例背景和目标
- 056 样本不平衡解决方案
- 057 下采样策略
- 058 交叉验证
- 059 模型评估方法
- 060 正则化惩罚项
- 061 逻辑回归函数
- 062 混淆矩阵
- 063 逻辑回归阈值对结果的影响
- 064 SMOTE样本生成策略

### 第21章 降维算法:线性判别分析
- 108 线性判别分析要解决的问题
- 109 线性判别分析要优化的目标
- 110 线性判别分析求解

### 第23章 降维算法:PCA主成分分析
- 113 PCA降维概述
- 114 PCA要优化的目标
- 115 PCA求解
- 116 PCA降维实战